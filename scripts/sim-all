#!/usr/bin/env python

import upcycle as U
from dataclasses import dataclass
import os
import sys
import functools
import numpy as np
import logging
import multiprocessing
import time
import signal
import argparse
import tqdm
import yaml

blue = '\x1b[38;5;39m'
green = '\033[92m'
reset = '\x1b[0m'

RESULT_DIR = './results'

logging._srcfile = None
logger = logging.getLogger()
ch = logging.StreamHandler(sys.stdout)
fmt = U.logutils.CustomFormatter()
ch.setFormatter(fmt)
logging.basicConfig(level=logging.INFO, handlers=[ch])

logging.getLogger('upcycle.model.common.opcount').setLevel(logging.CRITICAL)

if __name__ == '__main__':
    os.makedirs(RESULT_DIR, exist_ok=True)
    counter = None
    lock = None
    cancel = None

def init_pool_processes(c, l, x):
    global counter
    global lock
    global cancel

    counter = c
    lock = l
    cancel = x

def signal_handler(signal, frame):
    print('Attempting to gracefully exit...')
    global cancel
    cancel.value = True

def simulate_layer(arch : U.Arch, op : U.ops.Operator, sim_kwargs):
    logger.debug(f'Simulating {op}...')
    global counter
    global lock
    global cancel
    sim_kwargs['counter'] = counter
    sim_kwargs['lock'] = lock
    sim_kwargs['cancel'] = cancel
    try:
        result = U.model.simulate(arch, op, **sim_kwargs)
    except KeyboardInterrupt:
        logger.warning(f'KeyboardInterrupt while simulating {op} on {arch}')
        logger.warning(f'Returning Null result')
        result = U.model.SimResult(0, 0, None, dict(
            l1_accesses=0,
            l2_accesses=0,
            llc_accesses=0
        ))
    logger.debug(f'Finished {op}')
    return result

def log_layer(arch : U.Arch, dtype : U.Dtype, app : U.apps.Trace, i, op : U.ops.Operator, result : U.model.SimResult, total_cyc, count):
    gops = int(op.flops / result.cycles * arch.freq / 1e12)
    eff = np.round(op.flops / result.cycles / arch.ntiles / arch.peak_opc(dtype) * 100, 2)
    cyc_frac = np.round(result.cycles / total_cyc * 100, 2) * count
    logger.info(f'Layer {i}/{len(app.oplist)}: {op}, {result.cycles} cyc ({cyc_frac} %) (AmI = {np.round(op.ami, 2)}), {green} {gops} TOP/s {reset} ({blue}Efficiency: {eff} %{reset})')

@dataclass
class YamlResult:
    simsec : float
    simsteps : int
    timeout : bool
    timeout_steps : int
    cycles : int
    l1_accesses : int
    l2_accesses : int
    llc_accesses : int
    total_read_bytes : int
    total_weight_bytes : int
    total_write_bytes : int

    @staticmethod
    def from_sim_result(op : U.Operator, result : U.model.SimResult):
        return YamlResult(
            simsec=result.simsec,
            simsteps=result.nsteps,
            timeout=result.kwstats['timeout'],
            timeout_steps=result.kwstats['timeout-steps'],
            cycles=result.cycles,
            l1_accesses=result.kwstats['l1_accesses'],
            l2_accesses=result.kwstats['l2_accesses'],
            llc_accesses=result.kwstats['llc_accesses'],
            total_read_bytes=op.total_read_bytes,
            total_weight_bytes=op.total_weight_bytes,
            total_write_bytes=op.total_write_bytes
        )

    @staticmethod
    def from_existing_yaml(data):
        return YamlResult(
            simsec=data['simsec'],
            simsteps=1 if 'simsteps' not in data else data['simsteps'],
            timeout=False if 'timeout' not in data else data['timeout'],
            timeout_steps=0 if 'timeout-steps' not in data else data['timeout-steps'],
            cycles=data['cycles'],
            l1_accesses=data['l1_accesses'],
            l2_accesses=data['l2_accesses'],
            llc_accesses=data['llc_accesses'],
            total_read_bytes=data['total_read_bytes'],
            total_weight_bytes=data['total_weight_bytes'],
            total_write_bytes=data['total_write_bytes']
        )

    def write_to_file(self, op, f):
        f.write(f'{op}:\n')
        f.write(f'    simsec: {self.simsec}\n')
        f.write(f'    simsteps: {self.simsteps}\n')
        f.write(f'    timeout: {self.timeout}\n')
        f.write(f'    timeout-steps: {self.timeout_steps}\n')
        f.write(f'    cycles: {self.cycles}\n')
        f.write(f'    l1_accesses: {self.l1_accesses}\n')
        f.write(f'    l2_accesses: {self.l2_accesses}\n')
        f.write(f'    llc_accesses: {self.llc_accesses}\n')
        f.write(f'    total_read_bytes: {self.total_read_bytes}\n')
        f.write(f'    total_weight_bytes: {self.total_weight_bytes}\n')
        f.write(f'    total_write_bytes: {self.total_write_bytes}\n')
        f.write('\n')

@functools.lru_cache(maxsize=32)
def load_existing_db(arch):
    filename = f'{RESULT_DIR}/{arch.keystr}.yaml'
    if not os.path.exists(filename): return dict()
    return yaml.load(open(f'{RESULT_DIR}/{arch.keystr}.yaml', 'r'), Loader=yaml.FullLoader)

def try_get_existing_result(arch, op):
    db = load_existing_db(arch)
    if repr(op) in db: return YamlResult.from_existing_yaml(db[repr(op)])
    return None

def simulate_apps_par(parallel : int, archs : list[U.Arch], apps : dict[str, U.apps.Trace], verbose=True):
    global counter
    global lock
    global cancel
    counter = multiprocessing.Value('i', 0)
    lock = multiprocessing.Lock()
    cancel = multiprocessing.Value('b', False)

    pool = multiprocessing.Pool(
        parallel, initializer=init_pool_processes, initargs=(counter, lock, cancel))
    tt0 = time.perf_counter_ns()

    unique_ops = sum(map(lambda app: list(app.unique_ops), apps.values()), [])

    arch_ops = [
        (arch, op)
        for arch in archs
        for op in unique_ops
    ]

    completed_arch_ops = []
    to_run_arch_ops = []

    for arch, op in arch_ops:
        r = try_get_existing_result(arch, op)
        if r is not None:
            if r.timeout:
                logger.warning(f'Existing result for {arch} {op} timed out. Rerunning...')
                to_run_arch_ops.append((arch, op))
            else:
                completed_arch_ops.append((arch, op))
        else:
            to_run_arch_ops.append((arch, op))


    logger.info(f'Counting steps for {len(to_run_arch_ops)} (arch, op)s...')
    arch_op_steps = list(pool.starmap(U.model.num_steps, to_run_arch_ops, chunksize=1))
    total_steps = sum(arch_op_steps)
    logger.info(f'Counted {total_steps} steps')

    i_total_steps = list(enumerate(arch_op_steps))
    i_total_steps.sort(key=lambda x: x[1], reverse=True)

    logger.info('')
    logger.info('Top 10 ops by steps:')
    for i, steps in i_total_steps[:10]:
        logger.info(f'{to_run_arch_ops[i][1]}: {steps} steps')
        logger.info(f'    arch={to_run_arch_ops[i][0]}')
    logger.info('')

    progress = tqdm.tqdm(total=total_steps, unit='steps', smoothing=0.05)

    result = pool.starmap_async(
        functools.partial(simulate_layer, sim_kwargs={}), to_run_arch_ops, chunksize=1)

    last = 0
    while not result.ready():
        result.wait(0.5)
        cur = counter.value
        progress.update(cur - last)
        last = cur

    progress.close()
    unique_results = result.get()

    all_results = []

    for (arch, op) in completed_arch_ops:
        r = try_get_existing_result(arch, op)
        all_results.append((arch, op, r))

    for (arch, op), r in zip(to_run_arch_ops, unique_results):
        all_results.append((arch, op, YamlResult.from_sim_result(op, r)))


    tt1 = time.perf_counter_ns()

    return tt1 - tt0, all_results

def record_result_to_yaml(f, op : U.Operator, result : U.model.SimResult):
    l1_accesses = result.kwstats['l1_accesses']
    l2_accesses = result.kwstats['l2_accesses']
    llc_accesses = result.kwstats['llc_accesses']
    timeout = result.kwstats['timeout']
    timeout_steps = result.kwstats['timeout-steps']

    f.write(f'{op}:\n')
    f.write(f'    simsec: {result.simsec}\n')
    f.write(f'    simsteps: {result.nsteps}\n')
    f.write(f'    timeout: {timeout}\n')
    f.write(f'    timeout-steps: {timeout_steps}\n')
    f.write(f'    cycles: {result.cycles}\n')
    f.write(f'    l1_accesses: {l1_accesses}\n')
    f.write(f'    l2_accesses: {l2_accesses}\n')
    f.write(f'    llc_accesses: {llc_accesses}\n')
    f.write(f'    total_read_bytes: {op.total_read_bytes}\n')
    f.write(f'    total_weight_bytes: {op.total_weight_bytes}\n')
    f.write(f'    total_write_bytes: {op.total_write_bytes}\n')
    f.write('\n')


apps : dict[str, U.apps.Trace] = {
    'resnet50-infer-online':  U.apps.mlperf_v1_apps['resnet50'].default_infer_online(),
    'resnet50-infer-offline': U.apps.mlperf_v1_apps['resnet50'].default_infer_offline(),
    'resnet50-train-small':   U.apps.mlperf_v1_apps['resnet50'].default_train_small(),
    'resnet50-train-large':   U.apps.mlperf_v1_apps['resnet50'].default_train_large(),

    'ssdrn34-infer-online':  U.apps.mlperf_v1_apps['ssdrn34-1200'].default_infer_online(),
    'ssdrn34-infer-offline': U.apps.mlperf_v1_apps['ssdrn34-1200'].default_infer_offline(),
    'ssdrn34-train-small':   U.apps.mlperf_v1_apps['ssdrn34-300'].default_train_small(),
    'ssdrn34-train-large':   U.apps.mlperf_v1_apps['ssdrn34-300'].default_train_large(),

    'unet-infer-online':  U.apps.mlperf_v1_apps['unet'].default_infer_online(),
    'unet-infer-offline': U.apps.mlperf_v1_apps['unet'].default_infer_offline(),
    'unet-train-small':   U.apps.mlperf_v1_apps['unet'].default_train_small(),
    'unet-train-large':   U.apps.mlperf_v1_apps['unet'].default_train_large(),

    'bert-infer-online':  U.apps.mlperf_v1_apps['bert-large-squad'].default_infer_online(),
    'bert-infer-offline': U.apps.mlperf_v1_apps['bert-large-squad'].default_infer_offline(),
    'bert-train-small':   U.apps.mlperf_v1_apps['bert-large-pretrain'].default_train_small(),
    'bert-train-large':   U.apps.mlperf_v1_apps['bert-large-pretrain'].default_train_large(),

    'rnnt-infer-online':  U.apps.mlperf_v1_apps['rnnt'].default_infer_online(),
    'rnnt-infer-offline': U.apps.mlperf_v1_apps['rnnt'].default_infer_offline(),
    'rnnt-train-small':   U.apps.mlperf_v1_apps['rnnt'].default_train_small(),
    'rnnt-train-large':   U.apps.mlperf_v1_apps['rnnt'].default_train_large(),
}

archs = [
    U.arch.arch_factory('hier', dict(
        vbits=vbits,
        tpeng=tpeng,
        geom=U.arch.ntiles_to_geom(ntiles),
        compute_scale=[0.5, 1, 2, 10, 100, 0],
        noc_scale=[0.5, 1, 2, 10, 100, 0]))

    for ntiles in [512, 1024, 2048, 4096]
    for vbits in [256, 512, 1024]
    for tpeng in [True]
]

if __name__ == '__main__':
    # signal.signal(signal.SIGINT, signal_handler)

    parser = argparse.ArgumentParser(description='Simulate an application')
    parser.add_argument('-p', '--parallel', type=int, default=1)
    parser.add_argument('-v', '--verbose', action='store_true')
    args = parser.parse_args()

    time_ns, results = simulate_apps_par(args.parallel, archs, apps, args.verbose)

    arch : U.Arch
    for arch in archs:
        with open(f'{RESULT_DIR}/{arch.keystr}.yaml', 'w') as f:
            for (a, op, r) in results:
                if a is arch: r.write_to_file(op, f)


